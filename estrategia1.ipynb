{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "from IPython.display import clear_output, display # Para plots dinamicos no notebook\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import os, h5py, torch, random, time, gc\n",
        "from threading import Thread, Semaphore\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn.functional as F\n",
        "from scipy.ndimage import zoom # Caso precise reduzir a imagem\n",
        "from logger import log\n",
        "import torch.nn as nn\n",
        "import seaborn as sns\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Testar funcionamento dos algorítmos (em algum momento o kernel pode quebrar e isso daqui verifica se quebrou ou não)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "P0gayq3v4rND"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = nn.Linear(10, 2).to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "criterion = nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D-UlSDNnNy81"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "SpectrogramCNN(\n",
              "  (conv_layers): Sequential(\n",
              "    (0): Conv2d(1, 128, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2))\n",
              "    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (2): LeakyReLU(negative_slope=0.01)\n",
              "    (3): AvgPool2d(kernel_size=3, stride=3, padding=0)\n",
              "    (4): Dropout(p=0.2, inplace=False)\n",
              "    (5): Conv2d(128, 128, kernel_size=(20, 100), stride=(1, 1), padding=(2, 2))\n",
              "    (6): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (7): LeakyReLU(negative_slope=0.01)\n",
              "    (8): AvgPool2d(kernel_size=3, stride=3, padding=0)\n",
              "    (9): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (fc_layers): Sequential(\n",
              "    (0): Linear(in_features=64, out_features=11, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class SpectrogramCNN(nn.Module):\n",
        "    def __init__(self, num_classes=10):\n",
        "        super(SpectrogramCNN, self).__init__()\n",
        "\n",
        "        conv1 = 128\n",
        "        conv2 = 128\n",
        "        conv3 = 16\n",
        "        conv4 = 16\n",
        "        conv5 = 16\n",
        "        conv6 = 16\n",
        "\n",
        "        first_linear = 323712\n",
        "        linear_multpl = 57\n",
        "        next_first_linear = int(first_linear*linear_multpl)\n",
        "\n",
        "        self.conv_layers = nn.Sequential(\n",
        "            nn.Conv2d(1, conv1, kernel_size=3, stride=1, padding=2),\n",
        "            nn.BatchNorm2d(conv1),\n",
        "            nn.LeakyReLU(negative_slope=0.01),\n",
        "            nn.AvgPool2d(3),\n",
        "            nn.Dropout(.2),\n",
        "\n",
        "            nn.Conv2d(conv1, conv2, kernel_size=(20, 100), stride=1, padding=2),\n",
        "            nn.BatchNorm2d(conv2),\n",
        "            nn.LeakyReLU(negative_slope=0.01),\n",
        "            nn.AvgPool2d(3),\n",
        "            nn.Dropout(.1),\n",
        "\n",
        "            # nn.Conv2d(conv2, conv3, kernel_size=3, stride=1, padding=2),\n",
        "            # nn.BatchNorm2d(conv3),\n",
        "            # nn.LeakyReLU(negative_slope=0.01),\n",
        "            # nn.AvgPool2d(3),\n",
        "            # nn.Dropout(.1),\n",
        "\n",
        "            # nn.Conv2d(conv3, conv4, kernel_size=3, stride=1, padding=2),\n",
        "            # nn.BatchNorm2d(conv4),\n",
        "            # nn.LeakyReLU(negative_slope=0.01),\n",
        "            # nn.AvgPool2d(3),\n",
        "            # nn.Dropout(.1),\n",
        "\n",
        "            # nn.Conv2d(conv4, conv5, kernel_size=3, stride=1, padding=2),\n",
        "            # nn.BatchNorm2d(conv5),\n",
        "            # nn.LeakyReLU(negative_slope=0.01),\n",
        "            # nn.AvgPool2d(3),\n",
        "            # nn.Dropout(.1),\n",
        "\n",
        "            # nn.Conv2d(conv5, conv6, kernel_size=3, stride=1, padding=2),\n",
        "            # nn.BatchNorm2d(conv6),\n",
        "            # nn.LeakyReLU(negative_slope=0.01),\n",
        "            # nn.AvgPool2d(3),\n",
        "            # nn.Dropout(.1),\n",
        "            \n",
        "        )\n",
        "\n",
        "        self.fc_layers = nn.Sequential(\n",
        "            nn.Linear(first_linear, num_classes)\n",
        "            # nn.Linear(first_linear, next_first_linear),\n",
        "            # nn.LeakyReLU(negative_slope=0.01),\n",
        "            # nn.Dropout(0.3),\n",
        "            # nn.Linear(next_first_linear, 128),\n",
        "            # nn.LeakyReLU(negative_slope=0.01),\n",
        "            # nn.Dropout(0.3),\n",
        "            # nn.Linear(128, num_classes)\n",
        "\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv_layers(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc_layers(x)\n",
        "        return x\n",
        "    \n",
        "\n",
        "caminho_das_musicas = \"./musicas\"\n",
        "dir_base = caminho_das_musicas\n",
        "dir_h5 = \"h5\"\n",
        "estilos = os.listdir(dir_base)\n",
        "ref_estilos = estilos.copy()\n",
        "modelo = SpectrogramCNN(len(ref_estilos))\n",
        "modelo.to(device)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Separar train de test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_test_h5_labels(): # Utilizei esse pois precisava economizar na memória, então embaralho os patchs e n os dados\n",
        "    dir_base = \"musicas\"\n",
        "    dir_h5 = \"h5\"\n",
        "\n",
        "    estilos = os.listdir(dir_base)\n",
        "    ref_estilos = estilos.copy()\n",
        "\n",
        "    h5_train_items_path = []\n",
        "    train_labels = []\n",
        "\n",
        "    h5_test_items_path = []\n",
        "    test_labels = []\n",
        "\n",
        "    for estilo in estilos:\n",
        "        if estilo == \"apenas_h5\" or \".rar\" in estilo:\n",
        "            continue\n",
        "        train_path = os.path.join(dir_base, estilo, dir_h5, \"train\")\n",
        "        test_path = os.path.join(dir_base, estilo, dir_h5, \"test\")\n",
        "\n",
        "        train_specs_names = os.listdir(train_path)\n",
        "        for h5_spec in train_specs_names:\n",
        "            h5_spec_path = os.path.join(train_path, h5_spec)\n",
        "            h5_train_items_path.append(h5_spec_path)\n",
        "            train_labels.append(ref_estilos.index(estilo))\n",
        "\n",
        "        test_spec_names = os.listdir(test_path)\n",
        "        for h5_spec in test_spec_names:\n",
        "            h5_spec_path = os.path.join(test_path, h5_spec)\n",
        "            h5_test_items_path.append(h5_spec_path)\n",
        "            test_labels.append(ref_estilos.index(estilo))\n",
        "\n",
        "    train_combinado = list(zip(h5_train_items_path, train_labels))\n",
        "    test_combinado = list(zip(h5_test_items_path, test_labels))\n",
        "\n",
        "    random.shuffle(train_combinado)\n",
        "    random.shuffle(test_combinado)\n",
        "\n",
        "    h5_train_items_path, train_labels = zip(*train_combinado)\n",
        "    h5_test_items_path, test_labels = zip(*test_combinado)\n",
        "\n",
        "    h5_train_items_path = list(h5_train_items_path)\n",
        "    train_labels = list(train_labels)\n",
        "\n",
        "    h5_test_items_path = list(h5_test_items_path)\n",
        "    test_labels = list(test_labels)\n",
        "\n",
        "    return h5_train_items_path, train_labels, h5_test_items_path, test_labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Ler h5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "def read_h5_data(h5_item, zoom_factor=None, time_cut=None):\n",
        "    with h5py.File(h5_item, \"r\") as f:\n",
        "        data = f[\"data\"]\n",
        "        data = data[:]\n",
        "        if time_cut is not None:\n",
        "            data = data[:, :, data.shape[2]//time_cut:]\n",
        "        \n",
        "        if zoom_factor is not None:\n",
        "            data = zoom(data, zoom_factor)\n",
        "\n",
        "        return data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Custom dataloader para RAM suficiente"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "class HighMemoryDataloader:\n",
        "\n",
        "    def __init__(self, train_urls, train_labels, test_urls, test_labels, batch_size, device, zoom_factor=None, time_cut=None):\n",
        "        self.train_urls: list = train_urls\n",
        "        self.train_labels: list = train_labels\n",
        "        self.test_urls: list = test_urls\n",
        "        self.test_labels: list = test_labels\n",
        "\n",
        "        self.device = device\n",
        "        self.batch_size = batch_size\n",
        "        self.zoom_factor = zoom_factor\n",
        "        self.time_cut = time_cut\n",
        "\n",
        "        self.actual_train_batch = 0\n",
        "        self.actual_test_batch = 0\n",
        "\n",
        "        self.max_train_batches = (len(train_urls)//batch_size) + 1\n",
        "        self.max_test_batches = (len(test_urls)//batch_size) + 1\n",
        "\n",
        "        self.ram_train_data = []\n",
        "        self.ram_train_labels = self.train_labels.copy()\n",
        "        self.ram_test_data = []\n",
        "        self.ram_test_labels = self.test_labels.copy()\n",
        "        self.load_dataset()\n",
        "\n",
        "    def reconfig_batch(self, new_batch_size):\n",
        "        self.batch_size  = new_batch_size\n",
        "\n",
        "        self.max_train_batches = (len(self.train_urls)//self.batch_size) + 1\n",
        "        self.max_test_batches = (len(self.test_urls)//self.batch_size) + 1\n",
        "\n",
        "        self.actual_train_batch = 0\n",
        "        self.actual_test_batch = 0\n",
        "\n",
        "    def load_dataset(self):\n",
        "        for i, t_url in enumerate(self.train_urls):\n",
        "            i = i+1\n",
        "            log(\n",
        "                \"Carregando variáveis de treino\",\n",
        "                f\"{i}/{len(self.train_urls)}\",\n",
        "                i,\n",
        "                len(self.train_urls)\n",
        "            )\n",
        "            data = read_h5_data(t_url, self.zoom_factor, self.time_cut)\n",
        "            self.ram_train_data.append(data)\n",
        "\n",
        "        log(\n",
        "            \"Carregando variáveis de treino\",\n",
        "            f\"{i}/{len(self.train_urls)}\",\n",
        "            i,\n",
        "            len(self.train_urls),\n",
        "            end=\"\\n\"\n",
        "        )\n",
        "\n",
        "        for i, t_url in enumerate(self.test_urls):\n",
        "            i = i+1\n",
        "            log(\n",
        "                \"Carregando variáveis de teste\",\n",
        "                f\"{i}/{len(self.test_urls)}\",\n",
        "                i,\n",
        "                len(self.test_urls)\n",
        "            )\n",
        "            data = read_h5_data(t_url, self.zoom_factor, self.time_cut)\n",
        "            self.ram_test_data.append(data)\n",
        "        log(\n",
        "            \"Carregando variáveis de teste\",\n",
        "            f\"{i}/{len(self.test_urls)}\",\n",
        "            i,\n",
        "            len(self.test_urls),\n",
        "            end=\"\\n\"\n",
        "        )\n",
        "\n",
        "    def get_train_batch(self):\n",
        "        start_index = self.actual_train_batch*self.batch_size\n",
        "        end_index = start_index + self.batch_size\n",
        "        train_data = self.ram_train_data[start_index : end_index]\n",
        "        train_labels = self.ram_train_labels[start_index : end_index]\n",
        "        self.actual_train_batch = (self.actual_train_batch + 1) % (self.max_train_batches)\n",
        "\n",
        "        batch_end = self.actual_train_batch == 0\n",
        "        \n",
        "        return torch.tensor(np.array(train_data), device=self.device), torch.tensor(np.array(train_labels), device=self.device), len(train_data) < self.batch_size\n",
        "    \n",
        "    def get_test_batch(self):\n",
        "        start_index = self.actual_test_batch*self.batch_size\n",
        "        end_index = start_index + self.batch_size\n",
        "        test_data = self.ram_test_data[start_index : end_index]\n",
        "        test_labels = self.ram_test_labels[start_index : end_index]\n",
        "        self.actual_test_batch = (self.actual_test_batch + 1) % (self.max_test_batches)\n",
        "\n",
        "        batch_end = self.actual_test_batch == 0\n",
        "        \n",
        "        return torch.tensor(np.array(test_data), device=self.device), torch.tensor(np.array(test_labels), device=self.device), len(test_data) < self.batch_size\n",
        "\n",
        "    def train_len(self):\n",
        "        return len(self.ram_train_data)\n",
        "    \n",
        "    def test_len(self):\n",
        "        return len(self.ram_test_data)\n",
        "    \n",
        "    def shuffle(self):\n",
        "        pares = list(zip(self.ram_train_data, self.ram_train_labels))\n",
        "        random.shuffle(pares)\n",
        "        self.ram_train_data, self.ram_train_labels = zip(*pares)\n",
        "        self.ram_train_data = list(self.ram_train_data)\n",
        "        self.ram_train_labels = list(self.ram_train_labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Custom dataloader com threads\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "class ThreadDataloader:\n",
        "\n",
        "  def __init__(self, train_urls: list, train_labels, test_urls: list, test_labels, batch_size, device, batch_multipl=3, zoom_factor=None, time_cut=None):\n",
        "    self.train_urls = train_urls\n",
        "    self.train_labels = train_labels\n",
        "    self.test_urls = test_urls\n",
        "    self.test_labels = test_labels\n",
        "\n",
        "    self.zoom_factor = zoom_factor\n",
        "    self.time_cut = time_cut\n",
        "    self.stop_threads = False\n",
        "    self.device = device\n",
        "\n",
        "    self.loaded_train_data = []\n",
        "    self.loaded_test_data = []\n",
        "    self.loaded_train_labels = []\n",
        "    self.loaded_test_labels = []\n",
        "\n",
        "    self.batch_size = batch_size\n",
        "    self.max_train_batches = len(train_urls)//batch_size\n",
        "    self.max_test_batches = len(test_urls)//batch_size\n",
        "    self.actual_train_batch = 0\n",
        "    self.actual_test_batch = 0\n",
        "    self.max_len = self.batch_size*batch_multipl\n",
        "    self.train_semas = Semaphore(len(train_urls))\n",
        "    self.test_semas = Semaphore(len(test_urls))\n",
        "    self.mutex = Semaphore()\n",
        "\n",
        "  def free_train(self):\n",
        "    self.stop_threads = True\n",
        "    self.mutex.release()\n",
        "    self.train_semas.release()\n",
        "\n",
        "  def free_test(self):\n",
        "    self.stop_threads = True\n",
        "    self.mutex.release()\n",
        "    self.test_semas.release()\n",
        "\n",
        "\n",
        "  def stop(self):\n",
        "    self.free_train()\n",
        "    self.free_test()\n",
        "\n",
        "  def collect_train(self):\n",
        "    while not self.stop_threads:\n",
        "      for train_url, train_label in zip(self.train_urls, self.train_labels):\n",
        "        if self.stop_threads:\n",
        "          break\n",
        "        data = read_h5_data(train_url, self.zoom_factor, self.time_cut)\n",
        "        self.mutex.acquire()\n",
        "        self.loaded_train_data.append(data)\n",
        "        self.loaded_train_labels.append(train_label)\n",
        "        self.mutex.release()\n",
        "        self.train_semas.acquire()\n",
        "\n",
        "  def collect_test(self):\n",
        "    while not self.stop_threads:\n",
        "      for test_url, test_label in zip(self.test_urls, self.test_labels):\n",
        "        if self.stop_threads:\n",
        "          break\n",
        "        data = read_h5_data(test_url, self.zoom_factor, self.time_cut)\n",
        "        self.mutex.acquire()\n",
        "        self.loaded_test_data.append(data)\n",
        "        self.loaded_test_labels.append(test_label)\n",
        "        self.mutex.release()\n",
        "        self.test_semas.acquire()\n",
        "\n",
        "  def get_train_batch(self):\n",
        "    while len(self.loaded_train_data) < self.batch_size:\n",
        "      time.sleep(0.5)\n",
        "\n",
        "    self.mutex.acquire()\n",
        "    train_data = []\n",
        "    train_labels = []\n",
        "    for _ in range(self.batch_size):\n",
        "      train_data.append(self.loaded_train_data.pop(0))\n",
        "      train_labels.append(self.loaded_train_labels.pop(0))\n",
        "      self.train_semas.release()\n",
        "\n",
        "    self.mutex.release()\n",
        "    self.actual_train_batch = (self.actual_train_batch + 1) % (self.max_train_batches)\n",
        "\n",
        "    batch_end = self.actual_train_batch == 0\n",
        "\n",
        "    return torch.tensor(np.array(train_data), device=self.device), torch.tensor(np.array(train_labels), device=self.device), batch_end\n",
        "\n",
        "  def get_test_batch(self):\n",
        "    while len(self.loaded_test_data) < self.batch_size:\n",
        "      time.sleep(0.5)\n",
        "\n",
        "    self.mutex.acquire()\n",
        "    test_data = []\n",
        "    test_labels = []\n",
        "    for _ in range(self.batch_size):\n",
        "      test_data.append(self.loaded_test_data.pop(0))\n",
        "      test_labels.append(self.loaded_test_labels.pop(0))\n",
        "      self.test_semas.release()\n",
        "\n",
        "    self.mutex.release()\n",
        "    self.actual_test_batch = (self.actual_test_batch + 1) % (self.max_test_batches)\n",
        "\n",
        "    batch_end = self.actual_test_batch == 0\n",
        "\n",
        "    return torch.tensor(np.array(test_data), device=self.device), torch.tensor(np.array(test_labels), device=self.device), batch_end\n",
        "\n",
        "  def train_len(self):\n",
        "   self.mutex.acquire()\n",
        "   length = len(self.loaded_train_data)\n",
        "   self.mutex.release()\n",
        "   return length\n",
        "\n",
        "  def test_len(self):\n",
        "   self.mutex.acquire()\n",
        "   length = len(self.loaded_test_data)\n",
        "   self.mutex.release()\n",
        "   return length\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Função de treino"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_step(modelo: SpectrogramCNN, dataloader: ThreadDataloader | HighMemoryDataloader):\n",
        "    modelo.train()\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    y_true = []\n",
        "    y_pred = []\n",
        "    stop_train = False\n",
        "\n",
        "    while not stop_train:\n",
        "        if isinstance(dataloader, ThreadDataloader):\n",
        "            while len(dataloader.loaded_train_data) < dataloader.batch_size:\n",
        "                log(\n",
        "                    f\"Treino {dataloader.actual_train_batch}/{dataloader.max_train_batches}\",\n",
        "                    f\"Coletando {dataloader.train_len()} items carregados em memória - Loss: {total_loss/(dataloader.actual_train_batch if dataloader.actual_train_batch != 0 else dataloader.max_train_batches)}\",\n",
        "                    dataloader.actual_train_batch,\n",
        "                    dataloader.max_train_batches,\n",
        "                    progress_char=\"+\",\n",
        "                    void_char=\" \"\n",
        "                )\n",
        "                time.sleep(0.5)\n",
        "\n",
        "        batch_data, batch_labels, stop_train = dataloader.get_train_batch()\n",
        "        # batch_data = batch_data.permute(0, 2, 1, 3)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = modelo(batch_data)\n",
        "        loss = criterion(outputs, batch_labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "        total += batch_labels.size(0)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        correct += (predicted == batch_labels).sum().item()\n",
        "\n",
        "        y_true.extend(batch_labels.cpu().numpy())\n",
        "        y_pred.extend(predicted.cpu().numpy())\n",
        "\n",
        "        log(\n",
        "          f\"Treino {dataloader.actual_train_batch}/{dataloader.max_train_batches}\",\n",
        "          f\"{len(batch_data)} batch_size - Loss: {total_loss/(dataloader.actual_train_batch if dataloader.actual_train_batch != 0 else dataloader.max_train_batches)}\",\n",
        "          dataloader.actual_train_batch,\n",
        "          dataloader.max_train_batches,\n",
        "          progress_char=\"+\",\n",
        "          void_char=\" \"\n",
        "        )\n",
        "\n",
        "    avg_loss = total_loss / dataloader.max_train_batches\n",
        "    log(\n",
        "\t\tf\"Treino {dataloader.actual_train_batch}/{dataloader.max_train_batches}\",\n",
        "\t\tf\"{len(batch_data)} batch_size - Loss: {total_loss/dataloader.max_train_batches}\",\n",
        "\t\tdataloader.actual_train_batch,\n",
        "\t\tdataloader.max_train_batches,\n",
        "\t\tprogress_char=\"+\",\n",
        "\t\tvoid_char=\" \"\n",
        "\t)\n",
        "\n",
        "    return avg_loss, y_true, y_pred, 100 * correct / total"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Função de teste"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "def test_step(modelo: SpectrogramCNN, dataloader: ThreadDataloader | HighMemoryDataloader):\n",
        "    modelo.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    total_loss = 0\n",
        "    y_true = []\n",
        "    y_pred = []\n",
        "    stop_test = False\n",
        "\n",
        "    while not stop_test:\n",
        "        with torch.no_grad():\n",
        "            if isinstance(dataloader, ThreadDataloader):\n",
        "                while len(dataloader.loaded_test_data) < dataloader.batch_size:\n",
        "                    log(\n",
        "                        f\"Teste {dataloader.actual_test_batch}/{dataloader.max_test_batches}\",\n",
        "                        f\"Coletando {dataloader.test_len()} items carreagados em memória - Loss {total_loss/(dataloader.actual_test_batch if dataloader.actual_test_batch != 0 else dataloader.max_test_batches)}\",\n",
        "                        dataloader.actual_test_batch,\n",
        "                        dataloader.max_test_batches,\n",
        "                        progress_char=\"H\",\n",
        "                        void_char=\"-\"\n",
        "                    )\n",
        "                    time.sleep(0.5)\n",
        "            \n",
        "            batch_data, batch_labels, stop_test = dataloader.get_test_batch()\n",
        "            # batch_data = batch_data.permute(0, 2, 1, 3)\n",
        "            outputs = modelo(batch_data)\n",
        "            loss = criterion(outputs, batch_labels)\n",
        "            total_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += batch_labels.size(0)\n",
        "            correct += (predicted == batch_labels).sum().item()\n",
        "\n",
        "            y_true.extend(batch_labels.cpu().numpy())\n",
        "            y_pred.extend(predicted.cpu().numpy())\n",
        "\n",
        "            log(\n",
        "                f\"Teste {dataloader.actual_test_batch}/{dataloader.max_test_batches}\",\n",
        "                f\"{len(batch_data)} batch_size - Loss {total_loss/(dataloader.actual_test_batch if dataloader.actual_test_batch != 0 else dataloader.max_test_batches)}\",\n",
        "                dataloader.actual_test_batch,\n",
        "                dataloader.max_test_batches,\n",
        "                progress_char=\"H\",\n",
        "                void_char=\"-\"\n",
        "            )\n",
        "\n",
        "    log(\n",
        "        f\"Teste {dataloader.actual_test_batch}/{dataloader.max_test_batches}\",\n",
        "        f\"{len(batch_data)} batch_size - Loss {total_loss/(dataloader.actual_test_batch if dataloader.actual_test_batch != 0 else dataloader.max_test_batches)}\",\n",
        "        dataloader.actual_test_batch,\n",
        "        dataloader.max_test_batches,\n",
        "        progress_char=\"H\",\n",
        "        void_char=\"-\"\n",
        "    )\n",
        "\n",
        "    avg_test_loss = total_loss / dataloader.max_test_batches\n",
        "    return avg_test_loss, y_true, y_pred, 100 * correct / total"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Treino e teste sendo efetuados com thead dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "# torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 842
        },
        "id": "LHtex5ktO4yh",
        "outputId": "3427ea08-450b-4adf-b433-6e842debe258"
      },
      "outputs": [],
      "source": [
        "# losses = []\n",
        "# accurracy_test = []\n",
        "# epochs = []\n",
        "# plt.ion()\n",
        "# fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(20, 5))\n",
        "# line_losses, = ax1.plot(epochs, losses, label='Train Loss')\n",
        "# line_accurracy_test, = ax2.plot(epochs, accurracy_test, label='Test Loss')\n",
        "\n",
        "\n",
        "# print(device)\n",
        "\n",
        "# zoom_factor = (1, 1, 1)\n",
        "# zoom_factor = None\n",
        "# batch_size = 15\n",
        "# time_cut = 2\n",
        "# time_cut = None\n",
        "\n",
        "# h5_train_items_path, train_labels, h5_test_items_path, test_labels = train_test_h5_labels()\n",
        "# dataloader = ThreadDataloader(h5_train_items_path, train_labels, h5_test_items_path, test_labels, batch_size, device, zoom_factor=zoom_factor, time_cut=time_cut, batch_multipl=5)\n",
        "# # dataloader.collect_train()\n",
        "# collect_train_thread = Thread(target=dataloader.collect_train)\n",
        "# collect_test_thread = Thread(target=dataloader.collect_test)\n",
        "# collect_train_thread.start()\n",
        "# collect_test_thread.start()\n",
        "\n",
        "# print(\"Aguardando 10s para acumulo inicial de dados\")\n",
        "# # time.sleep(10)\n",
        "\n",
        "# # print(modelo, flush=True)\n",
        "\n",
        "# optimizer = torch.optim.AdamW(modelo.parameters(), lr=0.001)\n",
        "# criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "# num_epochs = 1000\n",
        "# for epoch in range(1, num_epochs):\n",
        "#     print(f\"Epoca: {epoch}\", flush=True)\n",
        "#     if len(epochs) < 30:\n",
        "#         epochs.append(epoch+1)\n",
        "\n",
        "#     avg_loss = train_step(modelo, dataloader)\n",
        "#     losses.append(avg_loss)\n",
        "#     if len(losses) > 30:\n",
        "#         losses.pop(0)\n",
        "#     line_losses.set_xdata(epochs)\n",
        "#     line_losses.set_ydata(losses)\n",
        "#     ax1.relim()\n",
        "#     ax1.autoscale()\n",
        "#     ax1.set_title(\"Training Loss\")\n",
        "#     ax1.set_xlabel(\"Epoch\")\n",
        "#     ax1.set_ylabel(\"Loss\")\n",
        "\n",
        "#     clear_output(wait=True)\n",
        "#     display(fig)\n",
        "\n",
        "#     total_loss, y_true, y_pred, correct, total = test_step(modelo, dataloader)\n",
        "\n",
        "#     avg_test_loss = total_loss / len(h5_test_items_path)\n",
        "#     accuracy = 100 * correct / total\n",
        "#     accurracy_test.append(accuracy)\n",
        "#     if len(accurracy_test) > 30:\n",
        "#         accurracy_test.pop(0)\n",
        "#     line_accurracy_test.set_xdata(epochs)\n",
        "#     line_accurracy_test.set_ydata(accurracy_test)\n",
        "#     ax2.relim()\n",
        "#     ax2.autoscale()\n",
        "#     ax2.set_title(\"Testing accuracy\")\n",
        "#     ax2.set_xlabel(\"Epoch\")\n",
        "#     ax2.set_ylabel(\"Accuracy\")\n",
        "\n",
        "#     ax3.clear()\n",
        "#     ax3.set_title(\"Confusion Matrix\")\n",
        "\n",
        "#     cm = confusion_matrix(y_true, y_pred)\n",
        "#     sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", ax=ax3,\n",
        "#                 xticklabels=ref_estilos, yticklabels=ref_estilos, cbar=False)\n",
        "#     ax3.set_xlabel(\"Predicted\")\n",
        "#     ax3.set_ylabel(\"True\")\n",
        "\n",
        "\n",
        "#     clear_output(wait=True)\n",
        "#     display(fig)\n",
        "\n",
        "#     print(f\"Epoch {epoch}, Loss:: Train {avg_loss:.4f} Test {avg_test_loss:.4f}\", flush=True)\n",
        "#     print(f\"Test Accuracy: {accuracy:.2f}%\", flush=True)\n",
        "#     torch.save(modelo.state_dict(), f\"modelo.pth\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "# dataloader.stop()\n",
        "# collect_train_thread.join()\n",
        "# collect_test_thread.join()\n",
        "# print(\"Threads finalizadas\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Treino e teste sendo efetuados com memoria suficientemente grande"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 2000x1000 with 6 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "train_losses = []\n",
        "test_losses = []\n",
        "\n",
        "accuracy_train = []\n",
        "accuracy_test = []\n",
        "\n",
        "epochs = []\n",
        "\n",
        "plt.ion()\n",
        "fig, ax = plt.subplots(2, 3, figsize=(20, 10))\n",
        "plt.subplots_adjust(hspace=0.5)\n",
        "(ax1, ax2, ax3), (ax4, ax5, ax6) = ax\n",
        "\n",
        "line_train_losses, = ax1.plot(epochs, train_losses, label='Train Loss')\n",
        "line_train_accuracy, = ax2.plot(epochs, accuracy_train, label=\"Train accuracy\")\n",
        "\n",
        "line_test_losses, = ax4.plot(epochs, test_losses, label='Test Loss')\n",
        "line_test_accuracy, = ax5.plot(epochs, accuracy_test, label='Test accuracy')\n",
        "\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Carregando variáveis de treino  [>>>>>>>>>>>>>>>>>>>>]  6165/6165                                                                                                                                                                                                                 \n",
            "Carregando variáveis de teste  [>>>>>>>>>>>>>>>>>>>>]  1545/1545                                                                                                                       \n"
          ]
        }
      ],
      "source": [
        "zoom_factor = (1, 1, 1)\n",
        "zoom_factor = None\n",
        "batch_size = 10\n",
        "time_cut = 2\n",
        "time_cut = None\n",
        "\n",
        "h5_train_items_path, train_labels, h5_test_items_path, test_labels = train_test_h5_labels()\n",
        "\n",
        "dataloader = HighMemoryDataloader(h5_train_items_path, train_labels, h5_test_items_path, test_labels, batch_size, device, zoom_factor=zoom_factor, time_cut=time_cut)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoca: 1\n"
          ]
        },
        {
          "ename": "RuntimeError",
          "evalue": "mat1 and mat2 shapes cannot be multiplied (20x323712 and 64x11)",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 23\u001b[39m\n\u001b[32m     20\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mEpoca: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m, flush=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     21\u001b[39m epochs.append(epoch+\u001b[32m1\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m avg_loss, y_true, y_pred, accuracy = \u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodelo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     25\u001b[39m train_losses.append(avg_loss)\n\u001b[32m     26\u001b[39m line_train_losses.set_xdata(epochs)\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 27\u001b[39m, in \u001b[36mtrain_step\u001b[39m\u001b[34m(modelo, dataloader)\u001b[39m\n\u001b[32m     24\u001b[39m \u001b[38;5;66;03m# batch_data = batch_data.permute(0, 2, 1, 3)\u001b[39;00m\n\u001b[32m     26\u001b[39m optimizer.zero_grad()\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m outputs = \u001b[43mmodelo\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     28\u001b[39m loss = criterion(outputs, batch_labels)\n\u001b[32m     29\u001b[39m loss.backward()\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/projeto_ia/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/projeto_ia/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 73\u001b[39m, in \u001b[36mSpectrogramCNN.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     71\u001b[39m x = \u001b[38;5;28mself\u001b[39m.conv_layers(x)\n\u001b[32m     72\u001b[39m x = x.view(x.size(\u001b[32m0\u001b[39m), -\u001b[32m1\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m73\u001b[39m x = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfc_layers\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     74\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/projeto_ia/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/projeto_ia/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/projeto_ia/venv/lib/python3.12/site-packages/torch/nn/modules/container.py:244\u001b[39m, in \u001b[36mSequential.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    242\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[32m    243\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m244\u001b[39m         \u001b[38;5;28minput\u001b[39m = \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    245\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/projeto_ia/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/projeto_ia/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/projeto_ia/venv/lib/python3.12/site-packages/torch/nn/modules/linear.py:125\u001b[39m, in \u001b[36mLinear.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    124\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m125\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[31mRuntimeError\u001b[39m: mat1 and mat2 shapes cannot be multiplied (20x323712 and 64x11)"
          ]
        }
      ],
      "source": [
        "optimizer = torch.optim.AdamW(modelo.parameters(), lr=0.001)\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "train_losses = []\n",
        "test_losses = []\n",
        "\n",
        "accuracy_train = []\n",
        "accuracy_test = []\n",
        "\n",
        "epochs = []\n",
        "\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "batch_size = 20\n",
        "dataloader: HighMemoryDataloader\n",
        "dataloader.reconfig_batch(batch_size)\n",
        "\n",
        "num_epochs = 1000\n",
        "for epoch in range(1, num_epochs):\n",
        "    print(f\"Epoca: {epoch}\", flush=True)\n",
        "    epochs.append(epoch+1)\n",
        "\n",
        "    avg_loss, y_true, y_pred, accuracy = train_step(modelo, dataloader)\n",
        "\n",
        "    train_losses.append(avg_loss)\n",
        "    line_train_losses.set_xdata(epochs)\n",
        "    line_train_losses.set_ydata(train_losses)\n",
        "\n",
        "    accuracy_train.append(accuracy)\n",
        "    print(line_train_accuracy)\n",
        "    line_train_accuracy.set_xdata(epochs)\n",
        "    line_train_accuracy.set_ydata(accuracy_train)\n",
        "    \n",
        "    if len(train_losses) >= 2:\n",
        "        line_train_losses.set_color(\"green\" if avg_loss <= train_losses[-2] else \"red\")\n",
        "        line_train_accuracy.set_color(\"green\" if accuracy >= accuracy_train[-2] else \"red\")\n",
        "\n",
        "\n",
        "    ax1.relim()\n",
        "    ax1.autoscale()\n",
        "    ax1.set_title(\"Training Loss\")\n",
        "    ax1.set_xlabel(\"Epoch\")\n",
        "    ax1.set_ylabel(\"Loss\")\n",
        "\n",
        "    ax2.relim()\n",
        "    ax2.autoscale()\n",
        "    ax2.set_title(\"Training Accuracy\")\n",
        "    ax2.set_xlabel(\"Epoc\")\n",
        "    ax2.set_ylabel(\"Accuracy\")\n",
        "\n",
        "    ax3.clear()\n",
        "    ax3.set_title(\"Training Confusion Matrix\")\n",
        "\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap=\"Blues\", ax=ax3, xticklabels=ref_estilos, yticklabels=ref_estilos, cbar=False)\n",
        "    ax3.set_xlabel(\"Predicted\")\n",
        "    ax3.set_ylabel(\"True\")\n",
        "\n",
        "    clear_output(wait=True)\n",
        "    display(fig)\n",
        "\n",
        "    avg_test_loss, y_true, y_pred, accuracy = test_step(modelo, dataloader)\n",
        "\n",
        "    test_losses.append(avg_test_loss)\n",
        "    line_test_losses.set_xdata(epochs)\n",
        "    line_test_losses.set_ydata(test_losses)\n",
        "\n",
        "    accuracy_test.append(accuracy)\n",
        "    line_test_accuracy.set_xdata(epochs)\n",
        "    line_test_accuracy.set_ydata(accuracy_test)\n",
        "\n",
        "    if len(accuracy_test) >= 2:\n",
        "        line_test_losses.set_color(\"green\" if avg_test_loss <= test_losses[-2] else \"red\")\n",
        "        line_test_accuracy.set_color(\"green\" if accuracy >= accuracy_test[-2] else \"red\")\n",
        "\n",
        "    ax4.relim()\n",
        "    ax4.autoscale()\n",
        "    ax4.set_title(\"Testing Loss\")\n",
        "    ax4.set_xlabel(\"Epoch\")\n",
        "    ax4.set_ylabel(\"Loss\")\n",
        "\n",
        "    ax5.relim()\n",
        "    ax5.autoscale()\n",
        "    ax5.set_title(\"Testing accuracy\")\n",
        "    ax5.set_xlabel(\"Epoch\")\n",
        "    ax5.set_ylabel(\"Accuracy\")\n",
        "\n",
        "    ax6.clear()\n",
        "    ax6.set_title(\"Testing Confusion Matrix\")\n",
        "\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", ax=ax6, xticklabels=ref_estilos, yticklabels=ref_estilos, cbar=False)\n",
        "    ax6.set_xlabel(\"Predicted\")\n",
        "    ax6.set_ylabel(\"True\")\n",
        "\n",
        "\n",
        "    clear_output(wait=True)\n",
        "    display(fig)\n",
        "\n",
        "    print(f\"Epoch {epoch}, Loss:: Train {avg_loss:.4f} Test {avg_test_loss:.4f}\", flush=True)\n",
        "    print(f\"Test Accuracy: {accuracy:.2f}%\", flush=True)\n",
        "    torch.save(modelo.state_dict(), f\"modelo.pth\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V28",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
